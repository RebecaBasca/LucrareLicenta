{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie a blocnotesului Licenta-alg ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1mlVH9jDOZ5J44U6AGyWHY9RhRA9xEjcd",
      "authorship_tag": "ABX9TyOspbnLmF5QYLaC+HD9Dkqo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RebecaBasca/LucrareLicenta/blob/main/Copie_a_blocnotesului_Licenta_alg_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algoritmul de învățare automată prezentat preia un set de date creat de [OSMH](https://osmhhelp.org/research) și antrenează modelul pentru a oferi rezultatul necesar proiectului\n",
        "\n",
        "## Scopul principal este returnarea unei valori binare care semnifică dacă un client ar trebui să înceapă un tratament și ședințte de terapie, pe baza răspunusrilor date de acesta într-un chestionar.\n",
        "\n",
        "Pașii folosiți în determinarea modelului se pot observa în secțiunile următoare."
      ],
      "metadata": {
        "id": "9VD-gX-MEOP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Librăriile folosite și încărcarea setului de date**"
      ],
      "metadata": {
        "id": "9_tqYl4Ma2zv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se realizeaza conexiunea cu Google Drive pentru a prelua setul de date utilizat și se inițializează train_df cu datele din fișierul CSV respectiv.\n"
      ],
      "metadata": {
        "id": "ZtYu9Y7sz4GO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from google.colab import drive\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "metadata": {
        "id": "dUx-ACKBc0ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = '1ZJhp6UnzkiStwLvKzgXDRrSsv7jVEoVu' \n",
        "\n",
        "#se preia fisierul cu setul de date din Google Drive\n",
        "\n",
        "download = drive.CreateFile({'id': file_id})\n",
        "\n",
        "download.GetContentFile('file.csv')\n",
        "df  = pd.read_csv(\"file.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "xzMqHGnQc69j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import randint\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import binarize, LabelEncoder, MinMaxScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, precision_recall_curve\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "\n",
        "from subprocess import check_output\n",
        "\n",
        "train_df = pd.read_csv('../content/drive/MyDrive/survey.csv')\n"
      ],
      "metadata": {
        "id": "_G5f0Gwpd-Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Curățarea datelor**\n"
      ],
      "metadata": {
        "id": "Vz5b5L2NbC-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datele sunt pregătite pentru analiză prin eliminarea sau modificarea datelor care sunt incorecte, incomplete, irelevante, duplicate sau formatate incorect."
      ],
      "metadata": {
        "id": "HLKVzmLoCJot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#se caută datele ce lipsesc din fisier\n",
        "total = train_df.isnull().sum().sort_values(ascending=False)\n",
        "percent = (train_df.isnull().sum()/train_df.isnull().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "missing_data.head(20)\n",
        "print(missing_data)"
      ],
      "metadata": {
        "id": "r26l9NYmeyFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_df = train_df.drop(['comments'], axis= 1)\n",
        "train_df = train_df.drop(['state'], axis= 1)\n",
        "train_df = train_df.drop(['Timestamp'], axis= 1)\n",
        "\n",
        "train_df.isnull().sum().max()\n",
        "train_df.head(5)"
      ],
      "metadata": {
        "id": "EoMQRiQ9e2NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sunt asociate valori numerice defaulft în funcție de tipul de date din tabel."
      ],
      "metadata": {
        "id": "HUY7HcfgeZWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "defaultInt = 0\n",
        "defaultString = 'NaN'\n",
        "defaultFloat = 0.0\n",
        "\n",
        "intFeatures = ['Age']\n",
        "stringFeatures = ['Gender', 'Country', 'self_employed', 'family_history', 'treatment', 'work_interfere',\n",
        "                 'no_employees', 'remote_work', 'tech_company', 'anonymity', 'leave', 'mental_health_consequence',\n",
        "                 'phys_health_consequence', 'coworkers', 'supervisor', 'mental_health_interview', 'phys_health_interview',\n",
        "                 'mental_vs_physical', 'obs_consequence', 'benefits', 'care_options', 'wellness_program',\n",
        "                 'seek_help']\n",
        "floatFeatures = []\n",
        "\n",
        "for feature in train_df:\n",
        "    if feature in intFeatures:\n",
        "        train_df[feature] = train_df[feature].fillna(defaultInt)\n",
        "    elif feature in stringFeatures:\n",
        "        train_df[feature] = train_df[feature].fillna(defaultString)\n",
        "    elif feature in floatFeatures:\n",
        "        train_df[feature] = train_df[feature].fillna(defaultFloat)\n",
        "    else:\n",
        "        print('Error: Feature %s not recognized.' % feature)\n",
        "train_df.head(5)   "
      ],
      "metadata": {
        "id": "sZreM5_je9r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coloana Gender este prelucrată încât să apară doar 3 valori distincte, pentru a simplifica procesul de codificare a datelor."
      ],
      "metadata": {
        "id": "m46G9Ohke0RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gender = train_df['Gender'].str.lower()\n",
        "\n",
        "gender = train_df['Gender'].unique()\n",
        "\n",
        "\n",
        "male_str = [\"male\", \"m\", \"male-ish\", \"maile\", \"mal\", \"male (cis)\", \"make\", \"male \", \"man\",\"msle\", \"mail\", \"malr\",\"cis man\", \"Cis Male\", \"cis male\"]\n",
        "trans_str = [\"trans-female\", \"something kinda male?\", \"queer/she/they\", \"non-binary\",\"nah\", \"all\", \"enby\", \"fluid\", \"genderqueer\", \"androgyne\", \"agender\", \"male leaning androgynous\", \"guy (-ish) ^_^\", \"trans woman\", \"neuter\", \"female (trans)\", \"queer\", \"ostensibly male, unsure what that really means\"]           \n",
        "female_str = [\"cis female\", \"f\", \"female\", \"woman\",  \"femake\", \"female \",\"cis-female/femme\", \"female (cis)\", \"femail\"]\n",
        "\n",
        "for (row, col) in train_df.iterrows():\n",
        "\n",
        "    if str.lower(col.Gender) in male_str:\n",
        "        train_df['Gender'].replace(to_replace=col.Gender, value='male', inplace=True)\n",
        "\n",
        "    if str.lower(col.Gender) in female_str:\n",
        "        train_df['Gender'].replace(to_replace=col.Gender, value='female', inplace=True)\n",
        "\n",
        "    if str.lower(col.Gender) in trans_str:\n",
        "        train_df['Gender'].replace(to_replace=col.Gender, value='trans', inplace=True)\n",
        "\n",
        "\n",
        "stk_list = ['A little about you', 'p']\n",
        "train_df = train_df[~train_df['Gender'].isin(stk_list)]\n",
        "\n",
        "print(train_df['Gender'].unique())"
      ],
      "metadata": {
        "id": "rW2jpxCqfFll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_df['Age'].fillna(train_df['Age'].median(), inplace = True)\n",
        "\n",
        "s = pd.Series(train_df['Age'])\n",
        "s[s<18] = train_df['Age'].median()\n",
        "train_df['Age'] = s\n",
        "s = pd.Series(train_df['Age'])\n",
        "s[s>120] = train_df['Age'].median()\n",
        "train_df['Age'] = s\n",
        "\n",
        "\n",
        "train_df['age_range'] = pd.cut(train_df['Age'], [0,20,30,65,100], labels=[\"0-20\", \"21-30\", \"31-65\", \"66-100\"], include_lowest=True)"
      ],
      "metadata": {
        "id": "Ew32advyfKDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['self_employed'] = train_df['self_employed'].replace([defaultString], 'No')\n",
        "print(train_df['self_employed'].unique())"
      ],
      "metadata": {
        "id": "8oTwuj_lfNaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['work_interfere'] = train_df['work_interfere'].replace([defaultString], 'Don\\'t know' )\n",
        "print(train_df['work_interfere'].unique())"
      ],
      "metadata": {
        "id": "vlc6Pn9zfQR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.** **Codificarea datelor**\n"
      ],
      "metadata": {
        "id": "K3onc3hXfYlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datele sunt codificate printr-o convenție stabilită alfabetic pentru a fi transformate în valori numerice, deoarece algoritmul necesita astfel de date pentru a realiza corect predicțiile."
      ],
      "metadata": {
        "id": "l7DXV8ioFaYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "labelDict = {}\n",
        "for feature in train_df:\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    le.fit(train_df[feature])\n",
        "    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "    train_df[feature] = le.transform(train_df[feature])\n",
        "    \n",
        "    labelKey = 'label_' + feature\n",
        "    labelValue = [*le_name_mapping]\n",
        "    labelDict[labelKey] =labelValue\n",
        "    \n",
        "for key, value in labelDict.items():     \n",
        "    print(key, value)\n",
        "\n",
        "#Eliminăm coloana Country din setul de date\n",
        "train_df = train_df.drop(['Country'], axis= 1)\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "qkiLIPs4fbKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "total = train_df.isnull().sum().sort_values(ascending=False)\n",
        "percent = (train_df.isnull().sum()/train_df.isnull().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "missing_data.head(20)\n",
        "print(missing_data)"
      ],
      "metadata": {
        "id": "SZ2JBrnDAp-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Scalarea și ajustarea datelor**"
      ],
      "metadata": {
        "id": "R22oNnZtBK4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scalarea datelor e procesul de normalizare a gamei de parametri dintr-un set de date, iar mai jos este scalată coloana responsabilă cu vârsta persoanelor.\n",
        "\n"
      ],
      "metadata": {
        "id": "ak81U2KRMczW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "train_df['Age'] = scaler.fit_transform(train_df[['Age']])\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "_syw99z_Aq7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = ['Age', 'Gender', 'family_history', 'seek_help', 'age_range']\n",
        "X = train_df[feature_cols]\n",
        "y = train_df.treatment\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "\n",
        "methodDict = {}\n",
        "rmseDict = ()"
      ],
      "metadata": {
        "id": "px6inNxWBoOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Reglajul datelor**\n",
        "\n",
        "**Evaluarea unui model de clasificare**. Această funcție va evalua:\n",
        "\n",
        "**Acuratețea clasificării**: procentul de predicții corecte\n",
        "\n",
        "**Acuratețe nulă**: precizie care ar putea fi atinsă prezicând întotdeauna cea mai frecventă clasă\n",
        "\n",
        "**Procentul valorilor de 1**\n",
        "\n",
        "**Procentul de zerouri**\n",
        "\n",
        "**Matrice de confuzie**: Tabel care descrie performanța unui model de clasificare"
      ],
      "metadata": {
        "id": "mqDvhlOUVGWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evalClassModel(model, y_test, y_pred_class, plot=False):\n",
        "    print('Accuracy:', metrics.accuracy_score(y_test, y_pred_class))\n",
        "    \n",
        "    print('Null accuracy:\\n', y_test.value_counts())\n",
        "    \n",
        "    print('Percentage of ones:', y_test.mean())\n",
        "    \n",
        "    print('Percentage of zeros:',1 - y_test.mean())\n",
        "    \n",
        "    print('True:', y_test.values[0:25])\n",
        "    print('Pred:', y_pred_class[0:25])\n",
        "    \n",
        "    confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
        "    TP = confusion[1, 1]\n",
        "    TN = confusion[0, 0]\n",
        "    FP = confusion[0, 1]\n",
        "    FN = confusion[1, 0]\n",
        "    \n",
        "    sns.heatmap(confusion,annot=True,fmt=\"d\") \n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "    \n",
        "   \n",
        "    model.predict_proba(X_test)[0:10, 1]\n",
        "  \n",
        "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    if plot == True:\n",
        "        plt.rcParams['font.size'] = 12\n",
        "        plt.hist(y_pred_prob, bins=8)\n",
        "        \n",
        "        plt.xlim(0,1)\n",
        "        plt.title('Histogram of predicted probabilities')\n",
        "        plt.xlabel('Predicted probability of treatment')\n",
        "        plt.ylabel('Frequency')\n",
        "    \n",
        "    \n",
        "    y_pred_prob = y_pred_prob.reshape(-1,1) \n",
        "    y_pred_class = binarize(y_pred_prob, threshold= 0.3)[0]\n",
        "    \n",
        "    print('First 10 predicted probabilities:\\n', y_pred_prob[0:10])\n",
        "    \n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
        "    if plot == True:\n",
        "        plt.figure()\n",
        "        \n",
        "        plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "        plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.0])\n",
        "        plt.rcParams['font.size'] = 12\n",
        "        plt.title('ROC curve for treatment classifier')\n",
        "        plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "        plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "    \n",
        "  \n",
        "    def evaluate_threshold(threshold):\n",
        "        print('Specificity for ' + str(threshold) + ' :', 1 - fpr[thresholds > threshold][-1])\n",
        "\n",
        "    predict_mine = np.where(y_pred_prob > 0.50, 1, 0)\n",
        "    confusion = metrics.confusion_matrix(y_test, predict_mine)\n",
        "    print(confusion)\n",
        "    \n",
        "    \n",
        "    \n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "mjWAC8OFBn61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reglare folosind cross validation score**\n"
      ],
      "metadata": {
        "id": "6EnJdcKcWEoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tuningCV(knn):\n",
        "    \n",
        "    k_range = list(range(1, 31))\n",
        "    k_scores = []\n",
        "    for k in k_range:\n",
        "        knn = KNeighborsClassifier(n_neighbors=k)\n",
        "        scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
        "        k_scores.append(scores.mean())\n",
        "    print(k_scores)\n",
        "    plt.plot(k_range, k_scores)\n",
        "    plt.xlabel('Value of K for KNN')\n",
        "    plt.ylabel('Cross-Validated Accuracy')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Q_Ez68iGWDhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reglarea datelor cu GridSearchCV**"
      ],
      "metadata": {
        "id": "6gQ3_aioWOrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tuningGridSerach(knn):\n",
        "    k_range = list(range(1, 31))\n",
        "    print(k_range)\n",
        "    \n",
        "    param_grid = dict(n_neighbors=k_range)\n",
        "    print(param_grid)\n",
        "    \n",
        "    grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\n",
        "\n",
        "    grid.fit(X, y)\n",
        "    \n",
        "    grid.grid_scores_\n",
        "    \n",
        "    print(grid.grid_scores_[0].parameters)\n",
        "    print(grid.grid_scores_[0].cv_validation_scores)\n",
        "    print(grid.grid_scores_[0].mean_validation_score)\n",
        "    \n",
        "    grid_mean_scores = [result.mean_validation_score for result in grid.grid_scores_]\n",
        "    print(grid_mean_scores)\n",
        "    \n",
        "    plt.plot(k_range, grid_mean_scores)\n",
        "    plt.xlabel('Value of K for KNN')\n",
        "    plt.ylabel('Cross-Validated Accuracy')\n",
        "    plt.show()\n",
        "    \n",
        "    print('GridSearch best score', grid.best_score_)\n",
        "    print('GridSearch best params', grid.best_params_)\n",
        "    print('GridSearch best estimator', grid.best_estimator_)"
      ],
      "metadata": {
        "id": "NynoflkkWDO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reglarea datelor cu RandomizedSearchCV**"
      ],
      "metadata": {
        "id": "z_z35hAcWXQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tuningRandomizedSearchCV(model, param_dist):\n",
        "    rand = RandomizedSearchCV(model, param_dist, cv=10, scoring='accuracy', n_iter=10, random_state=5)\n",
        "    rand.fit(X, y)\n",
        "    rand.cv_results_\n",
        "    \n",
        "    print('Rand. Best Score: ', rand.best_score_)\n",
        "    print('Rand. Best Params: ', rand.best_params_)\n",
        "    \n",
        "    best_scores = []\n",
        "    for _ in range(20):\n",
        "        rand = RandomizedSearchCV(model, param_dist, cv=10, scoring='accuracy', n_iter=10)\n",
        "        rand.fit(X, y)\n",
        "        best_scores.append(round(rand.best_score_, 3))\n",
        "    print(best_scores)"
      ],
      "metadata": {
        "id": "91JrCeN8WC5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Determinarea proprietăților cu impact mai mare asupra rezultatului folosind ExtraTreesClassifier**"
      ],
      "metadata": {
        "id": "IGXT9IGQKfUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forest = ExtraTreesClassifier(n_estimators=250,\n",
        "                              random_state=0)\n",
        "\n",
        "forest.fit(X, y)\n",
        "importances = forest.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
        "             axis=0)\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "labels = []\n",
        "for f in range(X.shape[1]):\n",
        "    labels.append(feature_cols[f])      \n",
        "    \n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(range(X.shape[1]), importances[indices],\n",
        "       color=\"r\", yerr=std[indices], align=\"center\")\n",
        "plt.xticks(range(X.shape[1]), labels, rotation='vertical')\n",
        "plt.xlim([-1, X.shape[1]])\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "We_kS87ABoCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**7. Evaluarea modelelor**\n"
      ],
      "metadata": {
        "id": "_YxSoWi9V_c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este evaluat modelul setului de date cu ajutorul mai multor algoritmi de învățare automată, dintre care regresia logistică, algoritmul k-nearest neighbors, arborele de decizie și random forest au avut cele mai bune rezultate."
      ],
      "metadata": {
        "id": "2y7fuC7qM8kI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "cycISJWYWbeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regresia logistică prezice un rezultat binar, cum ar fi da sau nu, pe baza observațiilor anterioare ale setului de date."
      ],
      "metadata": {
        "id": "mOLZIKzyh0jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logisticRegression():\n",
        "    logreg = LogisticRegression()\n",
        "    logreg.fit(X_train, y_train)\n",
        "    \n",
        "    y_pred_class = logreg.predict(X_test)\n",
        "    \n",
        "    accuracy_score = evalClassModel(logreg, y_test, y_pred_class, True)\n",
        "    \n",
        "    methodDict['Log. Regres.'] = accuracy_score * 100\n"
      ],
      "metadata": {
        "id": "H6TUOu90Wkaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logisticRegression()"
      ],
      "metadata": {
        "id": "zAIa1o_qXjSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNeighbors Classifier**"
      ],
      "metadata": {
        "id": "cCBS336XYHGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Knn():\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "    \n",
        "    k_range = list(range(1, 31))\n",
        "    weight_options = ['uniform', 'distance']\n",
        "\n",
        "    param_dist = dict(n_neighbors=k_range, weights=weight_options)\n",
        "    tuningRandomizedSearchCV(knn, param_dist)\n",
        "    \n",
        "    knn = KNeighborsClassifier(n_neighbors=27, weights='uniform')\n",
        "    knn.fit(X_train, y_train)\n",
        "    \n",
        "    y_pred_class = knn.predict(X_test)\n",
        "    \n",
        "    accuracy_score = evalClassModel(knn, y_test, y_pred_class, True)\n",
        "\n",
        "    methodDict['KNN'] = accuracy_score * 100"
      ],
      "metadata": {
        "id": "-t9oy8lUYHcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Knn()"
      ],
      "metadata": {
        "id": "HkdO0pQTYMIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree classifier**"
      ],
      "metadata": {
        "id": "o018nMFxYP4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def treeClassifier():\n",
        "    tree = DecisionTreeClassifier()\n",
        "    featuresSize = feature_cols.__len__()\n",
        "    param_dist = {\"max_depth\": [3, None],\n",
        "              \"max_features\": randint(1, featuresSize),\n",
        "              \"min_samples_split\": randint(2, 9),\n",
        "              \"min_samples_leaf\": randint(1, 9),\n",
        "              \"criterion\": [\"gini\", \"entropy\"]}\n",
        "    tuningRandomizedSearchCV(tree, param_dist)\n",
        "    \n",
        "\n",
        "    tree = DecisionTreeClassifier(max_depth=3, min_samples_split=8, max_features=4, criterion='entropy', min_samples_leaf=7)\n",
        "    tree.fit(X_train, y_train)\n",
        "    \n",
        "\n",
        "    y_pred_class = tree.predict(X_test)\n",
        "    \n",
        "    accuracy_score = evalClassModel(tree, y_test, y_pred_class, True)\n",
        "\n",
        "    methodDict['Tree clas.'] = accuracy_score * 100"
      ],
      "metadata": {
        "id": "GvvM-dN2YQQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treeClassifier()"
      ],
      "metadata": {
        "id": "u3y--iYeYcyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forests**\n"
      ],
      "metadata": {
        "id": "jS2xOqmAYib3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algoritmul random forest a avut cel mai bun randament și a fost ales pentru a reliza predictile necesare pe baza setului de date antrenat de acesta. Astfel, s-a salvat pe Google Drive fișierul finalized_model.sav, iar apoi s-a folosit în proiectul Flask pentru a utiliza datele din chestionarul de pe platformă."
      ],
      "metadata": {
        "id": "6_l3SP_-OUml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def randomForest():\n",
        "    forest = RandomForestClassifier(n_estimators = 20)\n",
        "\n",
        "    featuresSize = feature_cols.__len__()\n",
        "    param_dist = {\"max_depth\": [3, None],\n",
        "              \"max_features\": randint(1, featuresSize),\n",
        "              \"min_samples_split\": randint(2, 9),\n",
        "              \"min_samples_leaf\": randint(1, 9),\n",
        "              \"criterion\": [\"gini\", \"entropy\"]}\n",
        "    tuningRandomizedSearchCV(forest, param_dist)\n",
        "    \n",
        "    forest = RandomForestClassifier(max_depth = None, min_samples_leaf=8, min_samples_split=2, n_estimators = 20, random_state = 1)\n",
        "    my_forest = forest.fit(X_train, y_train)\n",
        "\n",
        "    filename = '../content/drive/MyDrive/finalized_model.sav'\n",
        "    pickle.dump(my_forest, open(filename, 'wb'))\n"
      ],
      "metadata": {
        "id": "BeBlyAEqYitj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomForest()"
      ],
      "metadata": {
        "id": "-4OljtMQYnMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Graficul succesului metodelor folosite**"
      ],
      "metadata": {
        "id": "CPYN-liQZpiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotSuccess():\n",
        "    s = pd.Series(methodDict)\n",
        "    s = s.sort_values(ascending=False)\n",
        "    plt.figure(figsize=(12,8))\n",
        "    #Colors\n",
        "    ax = s.plot(kind='bar') \n",
        "    for p in ax.patches:\n",
        "        ax.annotate(str(round(p.get_height(),2)), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
        "    plt.ylim([70.0, 90.0])\n",
        "    plt.xlabel('Method')\n",
        "    plt.ylabel('Percentage')\n",
        "    plt.title('Success of methods')\n",
        "     \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jlXiMJCaZsJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotSuccess()"
      ],
      "metadata": {
        "id": "h0yakUMGZuye"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}